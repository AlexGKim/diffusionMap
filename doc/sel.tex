\documentclass[preprint]{aastex}
\usepackage{amsmath,amssymb}
\usepackage{mathrsfs}
\bibliographystyle{plain}


\begin{document}

\title{Target Selection}
\author{Alex Kim}

The objective is to select a high-purity sample with a minimum number of correctly classified emission-line galaxies (ELGs) from a catalog of galaxies that contains for each object
its magnitude in a number of bands and perhaps other information (e.g.\ shape).  A smaller sample of galaxies with ELG/non-ELG classification is available to
inform the selection process.

Procedure to avoid overtraining:
\begin{itemize}
\item Reserve $x_V\%$ of full sample (maintaining ratio of ELG and non-ELGs) as the Validation set.
\item Split remaining sample into $N$ Voting sets.
\item Reserve $x_T\%$ of each Voting set  (again maintaining ratio of ELG and non-ELGs) as its Training set with the complement being its Reference set.
\item Optimize classification for each Voting set based on performance on the Training set. 
\item Assess results of all Voting sets for the Validation set.
\end{itemize}
For Random Forest classifiers is $N=1$ sufficient to avoid overtraining?  Is this overkill?

Procedure for classification within each Voting set:
\begin{itemize}
\item For each Voting set...
\item   For each object in the Training set...
\item     Perform separate Diffusion Map reparameterizations for ELGs, and for non-ELGs in the Reference set, for the weight tuning parameter $\epsilon$, and $n=1$.
The parameters for elements in the Training set are the ``distances'' between the galaxy and each of the objects in the Training set.  The ELG and non-ELGs are
diffused separately to remove connectivity of potential regions that overlap in the two samples.
\item Using the new parameterizations of all objects in Training set use Random Forest for classification.
\item Optimize ELG purity over $\epsilon$ and the parameters of random forest, with a boundary condition that $\ge n$ ELGs are correctly identified.
\end{itemize}

\end{document}