\documentclass[preprint]{aastex}
\usepackage{amsmath,amssymb}
\usepackage{mathrsfs}

\providecommand{\abs}[1]{\lvert#1\rvert}\providecommand{\norm}[1]{\lVert#1\rVert}

\bibliographystyle{plain}


\begin{document}

\title{Photometric Redshift Subsamples}
\author{Alex Kim}

\section{Introduction}

The determination of galaxy redshifts from broad-band photometry (photometric redshifts) can provide redshift estimates
for the large numbers of galaxies observed with multi-band imaging.  Most  galaxies do not have a
spectroscopic redshift, so the use of photometric redshifts enables a bread range of science.  
Given its importance, a diverse range of photometric-redshift algorithms are now available.

Without fine spectral resolution, photometric redshift estimates are less precise and easily less accurate than redshifts
determined from spectral features.  All algorithms to date exhibit non-Gaussian tails, referred to as catastrophic errors,
in their distribution of the difference between photometric and spectroscopic redshifts (redshift error).
These catastrophic errors limit
the statistical power of a single galaxy redshift, and the accuracy of the ensemble of photometric redshifts.

Although catastrophic errors plague the full ensemble of galaxies, there may be a subset of  galaxies identifiable through
photometry that is free of catastrophic errors and/or has small dispersion in redshift error.  This article suggests
criteria for defining such a subset, given a set of galaxies with
photometric and spectroscopic redshifts.  One criterion is that a galaxy must be similar to other galaxies in the sample,
under the assumption that ``singleton'' galaxies are less likely to have a robust photometric redshift.
The second criterion is that a galaxy must be dissimilar to galaxies that have catastrophic error, making the assumption
that such errors do not occur uniformly for all galaxies but preferentially in identifiable subclasses.
The quantitative definition of ``similar'' and ``dissimilar'' is based on distances between galaxies; a galaxy with small
distances to many other galaxies is considered similar, whereas a galaxy with large distances to the catastrophic outliers
is considered dissimilar.

Two distances are considered in this study.  The first is the Euclidian distance between points in color-magnitude
space.  The second is the distance of a particular choice for the non-linear reparameterization of color-magnitude space:
the Diffusion Map distance represents the connectivity between two points through a diffusion process on a graph.

\section{Data}
An older catalog of REDMAGIC galaxies was provided by Eduardo Rozo in the file {\it stripe82\_\-run\_\-redmagic-1.0-08.fit}.
The catalog contains a set of galaxies selected through selection criteria unknown to me and with photometric redshifts
also with provenance unknown to me.  For this study, we use the subset with photometric redshift $0.1\le z_{phot} < 0.8$
and a spectroscopic redshift $z_{spec}>0.01$.   The resulting set contains 18684 galaxies. 
The catalog includes $griz$ magnitudes for each galaxy.



\section{Identifying Galaxy Subsets With Robust Photo-$z$}
\subsection{Sample Splitting and $k$-Fold Cross-Validation}
Before entering into the details of our procedure for identifying the subset of galaxies with low redshift-error dispersion
we now briefly discuss $k$-fold cross-validation, the methodology used to avoid overtraining while maintaining a large fraction
of available galaxies to optimize the process.  The data is randomly divided so that 90\% are assigned to
the Training set and the remaining 10\% to the Test set.  The Training set is used in the construction of the
subset-selection procedure; the Test set is not used in this construction and is thus provides an assessment of the
selection procedure unsusceptible to overtraining.  In this article, the validation of the performance of galaxy selection
is based on the Test set.

The procedure for subset selection is optimized with the Training set using $k$-folding.  The Training set is divided
into $k=10$ groups of equal size (plus or minus one).  For each set of candidate values for the
free  parameters to be optimized, the optimization metric is calculated $k$ times.  Each calculation of the optimization metric
is derived from one of the $k$ groups used as a validation set, using the combined complementary $k-1$ groups to train
the procedure.  The average of the $k$ 
metrics is used as the metric for that set of candidate parameter values.  This metric is calculated for a range of candidate
parameter values, from which the optimal set is found.  The final procedure, the one applied to the Test set, uses
the optimal set of parameter values with the full Training set; in this manner the entire Training set is used with parameters
optimized with an out-of-sample set to avoid overtraining.

The procedure described in \S\ref{procedure:sec} is  run many times with different permutations
of training and validation sets.  During the $k$-folding, the procedure is run
on the different permutations of the combined $k-1$ group
serving as the training set and the one out-of-sample group as the validation set.  After parameter optimization, the
full Training set and Test set are used as the training and validation sets respectively.

\subsection{Procedure for Subset Selection}
\label{procedure:sec}
The goal is to define criteria that identify subsets from the galaxy catalog that have low photometric-redshift error dispersion.
We posit that poor photometric redshift determinations can be avoided by: 1) considering only galaxies 
that are similar to other galaxies in the parent distribution; 2) excluding galaxies that are similar galaxies in
the parent distribution that have catastrophic errors.

The quantitative definition of ``similar'' is as follows.  Galaxies are described by a set of features, for
example broadband magnitudes and colors.  A distance between galaxies in features can be chosen,
for example the Euclidian distance on color-magnitude space.  For a galaxy $i$, we define the weight  as
\begin{equation}
w_i \equiv \sum_{j \in \{Galaxies\}} \exp{\left(-d_{ij}/\sigma\right)},
\label{weight:eqn}
\end{equation}
where $d_{ij}$ is the distance between galaxies $i$ and $j$, and $\sigma$ is a characteristic
length scale.  The weight is high if the galaxy has small distance to many other galaxies, which corresponds
to the idea of  similar.  Conversely, a dissimilar galaxy has low weight, having large distances from the bulk
of other galaxies.  We choose the exponential function as having the nice properties of having a contribution of one
for galaxy pairs with identical features, and being smooth in feature space compared to square- and higher-order
exponentials.  Otherwise there is nothing particularly special about this choice: the results in this article are
specific to this definition of weight and the exploration of other functions is left for later work.

The first posit entails selecting validation galaxies with analogs in the training set.  A weight $w^O$
is introduced as in  Eqn.\ \ref{weight:eqn} with  $\{Galaxies\}$ being the training set. 
The length scale is made flexible by setting $\ln{\sigma} = l_O +
n_O\delta l_O$.  The values of $l_O$ and $\delta l_O$ correspond to the mean and the standard deviation
of the logarithm of the  smallest distances between galaxies training set (based on a Gaussian fit on the distribution
after cutting $>3\sigma$ outliers.)  Galaxies with weight $w^O>w_O$ are considered
to have an analog in the training set and are thus retained as a subset candidate.  The values of $n_O$
and $w_O$ are free parameters of
the procedure to be optimized.

The second posit entails deselecting validation galaxies with analogs that have catastrophic redshift error
in the training set.  In this case a weight $w^X$ is introduced as in in Eqn.\ \ref{weight:eqn}  with the $\{Galaxies\}$
as the training-set galaxies with catastrophic errors.  A catastrophic error is
defined as $\abs{z_{phot}-z_{spec}}> z_C$, where $z_C$ is a free parameter of the procedure.
The length scale is described as $\ln{\sigma} = l_X +
n_X\delta l_X$, with the instruction of another free parameter $n_X$ and
$l_X$ and $\delta l_X$ are determined as  $l_O$ and $\delta l_O$
except using smallest logarithmic distances between catastrophic outliers only.

As an ad hoc choice, we choose a target size for the subset as 10\% of the training set.
The subset is then defined as the galaxies with smallest valies of $w^X$ and with  $w^O>w_O$.

The metric calculated for the subset is the fourth moment of the redshift error. The fourth moment, and not the
second, is used to emphasize the contribution of the catastrophic outliers.  This metric is minimized
in the optimization of the free parameters.

To summarize, the procedure introduces the free parameters $n_O$, $n_X$ to account for the length scales
used in defining similarity between galaxies.  
The parameter $w_O$ defines whether a galaxy is considered
a singleton or is similar to galaxies in the training set.
The optimization is performed on a grid of values with $n_O, n_X \in \{-1,0,1\}$ and $w_O \in \{10,15,20,25\}$.
Recall that the parameters are trained on a test set that is smaller than the full Test set that is ultimately applied.
The length scale is described by the properties of the distribution of distances (through  $n_O$ and $n_X$)
as the distribution of absolute distances depends on sample size.
For a validation galaxy to have a training set analog does not require that it is a common occurrence in the
parent population, so absolute numbers are used for $w_O$.

\subsection{Distances}

% DM
%{'catastrophe_cut': 0.029999999999999999, 'eps_par': 1.0, 'mask_var': 1.0, 'outlier_cut': 15.0}
% 0.030  1.000  1.000 15.000 186.000000  0.036 -1.475e-04
% Color
%{'catastrophe_cut': 0.059999999999999998, 'mask_var': -1.0, 'outlier_cut': 25.0}
% 0.060  0.000 -1.000 25.000 186.000000  0.016 -3.300e-07

\end{document}